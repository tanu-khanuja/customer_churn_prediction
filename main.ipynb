{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read file\n",
    "df = pd.read_csv('churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data inspection\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic information of dataset\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of dataset\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 10000 customer entries with 14 multiple features. Out of these 14 features only 10 can have impact on customer churn. \n",
    "\n",
    "1. CreditScore— Customer with a higher credit score is less likely to leave the bank.\n",
    "2. Geography— A customer’s location can affect their decision to leave the bank.\n",
    "3. Gender— It’s interesting to explore whether gender plays a role in a customer leaving the bank.\n",
    "4. Age— This is certainly relevant, since older customers are less likely to leave their bank than younger ones.\n",
    "5. Tenure— Refers to the number of years that the customer has been a client of the bank. Normally, older clients are more loyal and less likely to leave a bank.\n",
    "6. Balance— Also a very good indicator of customer churn, as people with a higher balance in their accounts are less likely to leave the bank compared to those with lower balances.\n",
    "7. NumOfProducts— Refers to the number of products that a customer has purchased through the bank.\n",
    "8. HasCrCard— People with a credit card are less likely to leave the bank.\n",
    "9. IsActiveMember— Active customers are less likely to leave the bank.\n",
    "10. EstimatedSalary— People with lower salaries are more likely to leave the bank compared to those with higher salaries.\n",
    "\n",
    "Target vector:\n",
    "1. Exited— whether or not the customer left the bank.\n",
    "\n",
    "All other columns like RowNumber, CustomerID, Surname have no relation with customer churn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "The dataset was inspected to check for missing values, duplicates, and basic statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "\n",
    "df.drop(columns=['RowNumber', 'CustomerId', 'Surname'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical description of dataset\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No missing values present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Handle Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate entries\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No duplicate entries present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Renaming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename column 'Exited' to 'Churn'\n",
    "\n",
    "df.rename(columns={'Exited':'Churn'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Anaysis\n",
    "For Exploratory data analysis, we will graphically analyse: How many churned and churning wrt all other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Total Churning Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Churning customer distribution\n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "import seaborn as sns; sns.set_theme()\n",
    "\n",
    "churn_counts= df['Churn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(churn_counts, labels=['Not Churned','Churned'], autopct='%1.2f%%')\n",
    "plt.title('Total churn distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Majority of the customers (~80%) continue to use the service without churning. Only 20% churned.\n",
    "- We can say that the data is imbalanced. \n",
    "- We can use SMOTE later to make a balanced datset for accurate prediction and compare results with the originally imbalanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Gender distribution among all customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_counts = df['Gender'].value_counts()\n",
    "gender_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(gender_counts, labels=['Male', 'Female'], autopct='%1.2f%%')\n",
    "plt.title('Gender Distribution Across All Bank Customers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Gender Distribution among Churned and Not Churned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df, x='Churn', hue='Gender', multiple='dodge', binwidth=0.25)\n",
    "plt.title(\"Gender Distribution and Churning\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gender distribution reveals that the majority of customers are male, while more churned customers are female."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data= df, x= 'Age', hue='Churn', kde=True)\n",
    "plt.xlabel('Age of customers')\n",
    "plt.ylabel('Customer counts')\n",
    "plt.title('Customer Age dostribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data=df, x='Churn', y='Age', hue='Churn')\n",
    "plt.title(\"Age distribution and Churning\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram and violin plot show that churn rates are higher among customers aged 40-50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Geography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Geography'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another method to find value_counts\n",
    "df.groupby(['Geography']).count()['Churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='Geography', hue='Churn',  multiple='dodge')\n",
    "plt.xlabel('Customer Location')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.title('Customer Churning Distribution across Customer Location')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the customers are located in France. However, the highest number of customers churned are from Germany, which also has minimum number of customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CreditScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='CreditScore', hue='Churn', kde=True)\n",
    "plt.xlabel('Customer Credit Score')\n",
    "plt.ylabel('Customer Counts')\n",
    "plt.title('Customer Churning wrt Credit Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data=df, x= 'Churn', y= 'CreditScore', hue= 'Churn')\n",
    "plt.legend(title='Churning status', loc= 'upper center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit scores reflect the overall financial behaviour of a customer. Thus is surely can be a factor to predict churning. Some factors like older accounts, ontime loan/credit card payments etc increases the credit score of a customer. Which means, higher the credit score higher are the chances of stability, thus less chance of churning. The customers with low and high credit scores have lesser tendency of churning while the customers with intermediate credit score have higher tendency to churn. But the number of customers who did not churn also follow the same curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tenure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tenure'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='Tenure', hue='Churn', multiple='dodge', binwidth=0.5).legend(['Churned', 'Not Churned'])\n",
    "plt.title('Tenure wise churning distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customers with tenure very high (>9) seems less likely to churn. Same goes for the new customers. However, the customers with tenure 1-9 seem to churn highly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Bank Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bank balance\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x= 'Balance', hue='Churn', kde=True, multiple='dodge')\n",
    "plt.title(\"Balance and Churn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customers with zero bank balance are highest among churners. Whereas the customers with very high bank balance (>1.5L) are less likely to leave the bank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Number of Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='NumOfProducts', hue='Churn', multiple='dodge', binwidth=0.5)\n",
    "plt.title(\"Number of Products and Churn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Most customers have 1-2 products, and this range is also prevalent among churners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Has Credit Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df, x= 'HasCrCard', hue= 'Churn')\n",
    "plt.title(\"Has Credit Card and Churn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of customers possess credit cards. In both churn and non-churn groups, those with credit cards are more prevalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Is Active Member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df, x='IsActiveMember', hue= 'Churn')\n",
    "plt.title(\"Is Active Member and Churn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Churners seems to be less active. However, the active memebers churning is also considerable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Estimated Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df, x='EstimatedSalary', hue='Churn', multiple='dodge')\n",
    "plt.title(\"Estimated Salary and Churn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Churning pattern seems similar among all the salary ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "\n",
    "There are two columns Geography and Gender which are object type and need to be encoded. The type of encoding depends on the model we are going to use for prediction. The Categorical data is converted to numerical values so that our ML model can understand it.\n",
    "1. Linear models: For linear models numerical values has meaning- equivalent to their magnitude.Thus one-hot-encoding is used. For example, if male and female are encoded as 1 and 0, then it will take 1 > 0 and train model accordingly, which will obviously not give the correct result.\n",
    "2. DT, RF, XGBoosts have no problem with label encoding because they do not use order of categories directly.\n",
    "\n",
    "So, we are going to perform one-hot-encoding, so that we can fit on different models and compare the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to perform one hot encoding. Using pd.get_dummies and sklearn.preprocessing.OneHotEncoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One hot Econding using Pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding using pandas\n",
    "df_encoded_pd = pd.get_dummies(df, columns=['Geography', 'Gender']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One hot encoding using sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "sk_encoded = encoder.fit_transform(df[['Geography', 'Gender']])\n",
    "df_sk = pd.DataFrame(sk_encoded, columns=encoder.get_feature_names_out())\n",
    "df_encoded_sk = pd.concat([df.drop(columns=['Geography', 'Gender']), df_sk], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded_sk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "- We have column values that range widely different. For efficient model generation and convergence using Gradient decsent we must scale them to a uniform level. As the data range varies widely and units are also not the same, we will use StandaedScaler for normalization. Standard Scler or Z-score normalization makes the data distribution uniform for features. When we fit the model, it stores the mean and SD for each column. When we transform the data, it normalizes the columns by subtracting with their mean and dividing by their respective SD.\n",
    "\n",
    "- We will apply the normalization on features [CreditScore, Age, Balance, EstimatedSalary]. We have already encoded the categorical features, so there is no need to scale them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit_transform(df_encoded_pd[['CreditScore', 'Age', 'Balance', 'EstimatedSalary']])\n",
    "# model_transformed = scaler.transform(df_encoded_pd)\n",
    "\n",
    "normalized_df = pd.DataFrame(model, columns= scaler.get_feature_names_out())\n",
    "\n",
    "# Concat with other columns as well\n",
    "\n",
    "normalized_df_concat = pd.concat([df_encoded_pd.drop(columns=['CreditScore', 'Age', 'Balance', 'EstimatedSalary']),normalized_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df_concat.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=5, figsize=(20,12))\n",
    "\n",
    "axes = axes.flatten()  # Flatten axes to make readable in for loop\n",
    "for i, col in enumerate(normalized_df_concat.columns):\n",
    "    sns.histplot(data=normalized_df_concat, x=col, ax=axes[i], kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the features are scaled down to similar level although they were varying very widely before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.boxplot(normalized_df_concat)\n",
    "plt.xticks(rotation= 90)\n",
    "plt.title(\"Box Plot Representing Outliers for Each Feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the outliers seems reasonable and doesn't seem like noise which should be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find if our features are independent or not we will plot correlation matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "((normalized_df_concat.corr()>0.5) & (normalized_df_concat.corr()!=1)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(normalized_df_concat.corr(), cmap='coolwarm', annot=True, fmt='.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is no significant correlation among features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature matrix and Target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = normalized_df_concat.drop(columns='Churn')\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = normalized_df_concat['Churn']\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test-Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset into training and testing sets to ensure fair evaluation of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{X_train.shape=}, {y_train.shape=}\\n{X_test.shape=}, {y_test.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the Model\n",
    "\n",
    "Two models are employed for churn prediction:\n",
    "1. Decision tree\n",
    "2. Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For hyperparameter tuning we are using grid serach with cross-valiadtion. This will help us provide the optimal parameter with enhanced model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = {'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [5,10,15],\n",
    "        'random_state': [0,42]\n",
    "        }\n",
    "\n",
    "grid_tree= GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=grid, cv=5)\n",
    "\n",
    "grid_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_tree.best_params_  # Get best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(criterion='entropy', max_depth= 6, random_state=None).fit(X_train, y_train)  # Train model for best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree \n",
    "print(tree.plot_tree(dtree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_dtree = dtree.score(X_test, y_test)  # Accuracy of DT model\n",
    "print(f\"Accuracy of DT: {acc_dtree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dtree = dtree.predict(X_test)  # Predict y using DT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar hyperparameter tuning will be conducted for the Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "grid2 = { 'n_estimators': [100,150,200],\n",
    "         'max_depth': [5,10,15],\n",
    "           'criterion': ['gini', 'entropy'],\n",
    "           'random_state': [0, 42], \n",
    "           }\n",
    "\n",
    "grid_rf = GridSearchCV(estimator=RandomForestClassifier(), param_grid=grid2, cv=5)\n",
    "\n",
    "grid_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rf.best_params_  # Get best fitted hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=grid_rf.best_params_['n_estimators'],criterion= grid_rf.best_params_[\"criterion\"],max_depth= grid_rf.best_params_[\"max_depth\"], random_state= grid_rf.best_params_[\"random_state\"]).fit(X_train, y_train)  # Train model using best hyperparameters\n",
    "\n",
    "acc_rf = rf_model.score(X_test, y_test)\n",
    "print(f\"Accuracy of RF: {acc_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rf = rf_model.predict(X_test)  # Predicted y using RF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracies of all the models are as follows:\n",
    "\n",
    "1. Decision tree: 86.08%\n",
    "2. Random forest: 86.76%\n",
    "\n",
    "Accuracies of DT and RF are more or less similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Confusion matrix\n",
    "Confusion matrix shows the true positives, true negatives, false positives, and false negatives for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "figure, axes = plt.subplots(nrows=1,ncols=2,figsize=(12,5))\n",
    "\n",
    "cm_dtree = confusion_matrix(y_test, y_dtree)\n",
    "cm_rf = confusion_matrix(y_test, y_rf)\n",
    "\n",
    "sns.heatmap(cm_dtree, annot=True, fmt='g', cmap='coolwarm', ax=axes[0])\n",
    "axes[0].set_title('Confusion Matrix for Decision Tree Model')\n",
    "\n",
    "sns.heatmap(cm_rf, ax=axes[1], annot=True, fmt='g', cmap='coolwarm' )\n",
    "axes[1].set_title('Confusion Matrix for Random Forest Model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification Report\n",
    "\n",
    "- To get scores: Precision, Recall, and F1-Score. To understand the report, we should know that:\n",
    "  1. Lower the precision means higher is the chances of false positives.(i.e. predicting many churn even when not churned).\n",
    "  2. Lower the recall means higher is the chances of false negatives. (i.e. unable to predict the many churned ones as churned).\n",
    "  3. F1-score finds a balance between precision and recall. Lower F1 score means, it is struggling to balance between precision and recall.\n",
    "\n",
    "- Higher the metrics values, good is the prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "cr_dtree = classification_report(y_test, y_dtree, output_dict=True)\n",
    "print(f'Classification Report for Decision Tree Model:\\n {cr_dtree}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_rf = classification_report(y_test, y_rf, output_dict=True)\n",
    "print(f\"Classification Report for RF model: \\n{cr_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Customer churn prediction it is very important to not miss any class '1' prediction. This means that our recall should be high and FN should be low. \n",
    "But as per the classification report, the recall for class '1' is low in both the models- 0.43 and 0.46.\n",
    "The lower F1-score for class '1' also indicates difficulty while balancing the recall and precision. This may be due to the imbalanced data with nearly 80%-20% ratio between class '0' and class '1'. \n",
    "The Accuracy was observed higher due to the 80% of class '0' data. \n",
    "\n",
    "We will check if balancing the dataset may help improve the recall or not. For this we will perform SMOTE analsyis on our training data and then check various metrics on original test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm= SMOTE(random_state=42)\n",
    "X_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sm.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing SMOTE our training dataset is now balanced. Let's train the models again.\n",
    "\n",
    "So, our data includes:\n",
    "1. training data: X_train_sm, y_train_sm\n",
    "2. testing data: X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retraining models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Decision tree smote model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search cv is used for hyperparameter tuning. The training dataset is changed based on SMOTE analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': range(2,15),\n",
    "        'random_state': [0,42]\n",
    "        }\n",
    "\n",
    "grid_tree_sm= GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=grid, cv=5)\n",
    "\n",
    "grid_tree_sm.fit(X_train_sm, y_train_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_tree_sm.best_params_  # Get optimized hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree_sm = DecisionTreeClassifier(criterion= grid_tree_sm.best_params_[\"criterion\"],max_depth= grid_tree_sm.best_params_[\"max_depth\"], random_state= grid_tree_sm.best_params_[\"random_state\"]).fit(X_train_sm, y_train_sm)  # Train model based on optimized hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_dtree_smote = dtree_sm.score(X_test, y_test)  # Accuracy of DT SMOTE model\n",
    "acc_dtree_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dtree_sm = dtree_sm.predict(X_test)  # y predicted using DT SMOTE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "cr_dtree_smote = classification_report(y_dtree_sm, y_test, output_dict=True)\n",
    "\n",
    "print(cr_dtree_smote)  # Recall, precision, and f1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, random forest model is also trained with smote training dataset along with hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid2 = {'max_depth': range(2,15),\n",
    "           'criterion': ['gini', 'entropy'],\n",
    "           'random_state': [0,42],\n",
    "           'n_estimators':[100,150,200] \n",
    "           }\n",
    "\n",
    "grid_rf_sm = GridSearchCV(RandomForestClassifier(), param_grid=grid2, cv=5)\n",
    "\n",
    "grid_rf_sm.fit(X_train_sm, y_train_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rf_sm.best_params_ # Get optimized hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_sm = RandomForestClassifier(n_estimators=grid_rf_sm.best_params_[\"n_estimators\"],criterion= grid_rf_sm.best_params_[\"criterion\"],max_depth= grid_rf_sm.best_params_[\"max_depth\"], random_state= grid_rf_sm.best_params_[\"random_state\"]).fit(X_train_sm, y_train_sm) # Train model based on optimized hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rf_sm = rf_sm.predict(X_test)  # y predicted using RF SMOTE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_rf_smote = classification_report(y_rf_sm, y_test, output_dict=True)  # Recall, precision, and f1 score\n",
    "print(cr_rf_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_rf_smote = rf_sm.score(X_test, y_test)  # Accuracy of RF smote model\n",
    "acc_rf_smote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant metrics\n",
    "data = {\n",
    "    'Model': ['Decision Tree', 'Random Forest', 'Decision Tree (SMOTE)', 'Random Forest (SMOTE)'],\n",
    "    'Precision': [cr_dtree['1']['precision'], \n",
    "                  cr_rf['1']['precision'], \n",
    "                  cr_dtree_smote['1']['precision'], \n",
    "                  cr_dtree_smote['1']['precision']],\n",
    "    'Recall': [cr_dtree['1']['recall'], \n",
    "               cr_rf['1']['recall'], \n",
    "               cr_dtree_smote['1']['recall'], \n",
    "               cr_rf_smote['1']['recall']],\n",
    "    'F1-Score': [cr_dtree['1']['f1-score'], \n",
    "                 cr_rf['1']['f1-score'], \n",
    "                 cr_dtree_smote['1']['f1-score'], \n",
    "                 cr_rf_smote['1']['f1-score']],\n",
    "    'Accuracy': [acc_dtree,\n",
    "                acc_rf,\n",
    "                acc_dtree_smote,\n",
    "                acc_rf_smote]\n",
    "    \n",
    "   \n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame(data)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Precision tells us how good the model is at making correct positive predictions:\n",
    "\n",
    "- The Random Forest model shines here, correctly predicting 78% of churners. It’s a reliable choice when it does say someone is likely to churn.\n",
    "\n",
    "Recall shows how many actual churners the model can spot:\n",
    "- The Random Forest with SMOTE does best in this category, catching 57.4% of actual churners. This means it's better at identifying customers who might leave.\n",
    "\n",
    "F1-Score balances precision and recall:\n",
    "- The Random Forest with SMOTE gets the highest F1-score (60%), indicating it does a good job of balancing being accurate and catching churners.\n",
    "\n",
    "Accuracy reflects overall correct predictions:\n",
    "- Looking at all other metrics Random Forest (SMOTE) has slightly lower accuracy that its respective non-SMOTE model. But the higher recall and F1-score compensates for that.\n",
    "\n",
    "If we want to be sure about churn predictions, we will go with the Random Forest without SMOTE for its high precision. If spotting churners is our priority, the Random Forest with SMOTE is better. For a balanced approach, consider the Random Forest with SMOTE; it gives a good mix of catching churners while still being accurate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "libraries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
